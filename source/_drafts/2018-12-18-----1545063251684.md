---
title: "[머신러닝] 스팀잇 글 분류하기 (첫번째 시도)"
author: anpigon
date: "2018-12-17T16:14:12Z"
permalink: "/kr/@anpigon/--1545063251684"
tags:
  - "kr"
  - "dev"
  - "kr-dev"
  - "jjangjjangman"
  - "busy"
---
스팀잇을 처음 시작했을 때부터 게시글을 카테고리별로 분류하고 싶었습니다. 머신러닝을 이용해서 하고 싶었던 프로젝트 중의 하나였어요. 그런데 머신러닝이 어려워서 한동안 손을 놓고 있었네요.

요즘은 머신러닝보다 리액트의 매력에 빠져서 리액트 코딩 영상과 문서만 보고 있습니다. ㅎㅎ 제가 머신러닝을 경험하면서 배운 점은 코딩과 머신러닝은 다른 세계라는 것입니다. 저같은 코더에게 머쉰러닝은 너무 어렵습니다. 

이 글은 사실 예전에 구현했다가 실패해서 draft에 담아두고 있던 글인데, 버리기 아까워서 다시 꺼냈습니다. 스팀잇 글을 분류하기 위해서 몇달 전에 포스팅했던 [머신러닝 나이브 베이즈 분류기](https://steemit.com/kr/@anpigon/4)를 사용하였습니다. **다시 한 번 결과를 말씀드리면 스팀잇 글을 분류하는 데 실패했습니다.**

<br><center>* * *</center><br>

머신러닝 학습 데이터를 만들기 위해서 네이버 블로그 사이트를 크롤링하였습니다. 네이버 블로그 사이트의 각 카테고리에서 100건씩 글을 가져왔습니다.

![](https://cdn.steemitimages.com/DQmSDocUrgq2mVPzryC5y1AJPLuoKujimPvYbaTAyNyn7ho/％E1％84％89％E1％85％B3％E1％84％8F％E1％85％B3％E1％84％85％E1％85％B5％E1％86％AB％E1％84％89％E1％85％A3％E1％86％BA％202018-12-17％2023.40.53.png)


참고로 네이버 블로그는 31개의 카테고리로 분류하고 있습니다. 그리고 분류 코드는 이렇습니다.

```js
directorySeqs = ｛
  5: '책',
  6: '영화',
  7: '공연/전시',
  8: '미술',
  9: '드라마',
  10: '방송',
  11: '음악',
  12: '연예인',
  13: '만화',
  14: '일상',
  15: '육아/결혼',
  16: '반려동물',
  17: '좋은글',
  18: '패션/미용',
  19: '인테리어',
  20: '요리',
  21: '상품리뷰',
  22: '게임',
  23: '스포츠',
  24: '사진',
  25: '자동차',
  26: '취미',
  27: '국내여행',
  28: '세계여행',
  29: '맛집',
  30: 'IT',
  31: '사회',
  32: '건강',
  33: '경제',
  34: '교육',
  35: '외국어',
  36: '원예',
｝
```

<br><center>* * *</center><br>

# 분류기 만들기

`textblob.classifiers` 에서 나이브베이즈 분류기(`NaiveBayesClassifier`)를 **import** 합니다. 그리고 `konlpy.tag`에서 은전한닢(`Mecab`) 형태소 분석기도 생성합니다.

```python
from textblob.classifiers import NaiveBayesClassifier
from konlpy.tag import Mecab
pos_tagger = Mecab() # 형태소 분석기
```

<br><br>

# 학습 데이터 만들기

크롤링한 데이터를 사용하여 학습 데이터를 생성합니다. 한글만 남기고 다른 문자는 모두 제거합니다. 그리고 형태소를 분석합니다.

```python
import os

train_data = []
for root, dirs, files in os.walk('data'):
    # 파일 모두 읽기
    for fname in files:
        full_fname = os.path.join(root, fname) # 읽을 파일 전체 경로
        directorySeq = root.split('/')[1] # 디렉토리=카테고리
        text = open(full_fname, 'r').read() # 파일 읽기
        text = stripNotHangul(text) # 한글을 제외한 문자는 제거
        pos_tag = ['/'.join(token) for token in pos_tagger.pos(text)] # 형태소 분석
        data = (pos_tag,  directorySeq)
        train_data.append(data) # 학습 데이터에 추가하기
```
> 참고로 네이버에서 크롤링한 데이터는 정제되어 있지 않습니다. 그래서 데이터가 매우 지저분한 상태입니다. 예를 들면 HTML 태그 구조가 네이버 에디터 버전마다 다릅니다. 그리고 문장의 시작과 끝이 구분되지 않는 경우도 있습니다.

<br><br>

# 학습하기

학습 데이터(`train_data`)를 이용하여 머신러닝을 학습합니다.
```python
cl = NaiveBayesClassifier(train_data)
```
> 제 노트북에서는 학습하는데 10분 이상 걸렸습니다. 발열과 팬소음도 납니다. 학습하다가 가끔 파이썬 커널(kernel)이 죽는 경우도 있어요. ㅠㅠ

<br><br>

# 분류기준 살펴보기

```python
cl.show_informative_features()
```
![](https://ipfs.busy.org/ipfs/QmXGF64ccAGhTk9hK2tXESCUUQn876Bwdh7VM4xwwLgGoe)
> 분류 기준을 보면 텍스트에 `맛/NNG`이 포함되어 있으면 **29(맛집)** 일 확률이 **63.7％** 입니다. 그리고 `차량/NNG`이 포함되어 있으면 **25(자동차)** 일 확률이 **62.2％** 입니다. 그리고 `영화/NNG`가 포함되어 있으면 **6(영화)** 일 확률이 **62.1％** 입니다. 분류기준은 나무랄 데가 없네요. ㅎㅎ

<br><br>

# 정확도 확인하기

네이버에서 크롤링한 데이터 중에서 일부를 가지고 정확도를 테스트 해보았습니다. 정확도가 `26％` 로 매우 낮습니다. 

![](https://cdn.steemitimages.com/DQmRf8nCYW9Qy6Naq87XRd582XXkpgayKETCRxx2eK3kbGj/Screenshot％20(3).png)
> 얘가 26점을 받아왔어요. 학습이 부진하네요. 다른 학습지 공부라도 시켜야 할 듯...

<br><br>

# 스팀잇 글 분류하기

스팀잇 `kr` 태그에서 게시글 100개를 가져왔습니다.

```python
discussions = get_discussions('kr', 100)
```

<br>그리고 분류기로 스팀잇 글을 분류해보았습니다. 하지만, 스팀잇에서 가져온 글이 모두 요리로 분류되었습니다. ㅠㅠ 100건의 글 카테고리가 모두 **[요리]** 입니다. ㅋㅋㅋㅋㅋ

![](https://ipfs.busy.org/ipfs/QmaumxoCB5kLyNMyefEeT5YUzbrDwHfxQNjTZQFHzYBwBJ)

<br><center>* * *</center><br>

원인은 모르겠습니다. 왜 이렇게 분류했는지를 확인하는 방법도 모르겠습니다. 제 노트북도 모른다네요. 좀 더 연구를 해봐야지 원인을 알 수 있을 것 같습니다. 다음번에는 텐서플로우를 사용해서 시도해보려고 합니다. 

여기까지 읽어주셔서 감사합니다.






---

#####  <sub> **Sponsored ( Powered by [dclick](https://www.dclick.io) )** </sub>
[![dclick-imagead](https://s3.ap-northeast-2.amazonaws.com/dclick/image/glory7/1544187953824.png)](https://api.dclick.io/v1/c?x=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjIjoiYW5waWdvbiIsInMiOiItLTE1NDUwNjMyNTE2ODQiLCJhIjpbImktNTkiXSwidXJsIjoiaHR0cDovL3d3dy5nb29kc3BpbmUub3JnLyIsImlhdCI6MTU0NTA2MzI1MSwiZXhwIjoxODYwNDIzMjUxfQ.mu31idGnO4XkBm1AqOq9j1smvcpI_T5-n1SytBLLYBw)